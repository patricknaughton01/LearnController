%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{float}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage[font=small]{caption}

%\usepackage[ruled]{algorithm2e}
\usepackage{algorithm, algpseudocode}
%\SetKwComment{Comment}{$\triangleright$ }{}
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{etoolbox}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{svg}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\graphicspath{ {images/} }

\newrobustcmd*{\bftabnum}{%
	\bfseries
	\sisetup{output-decimal-marker={\textmd{.}}}%
}

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\title{\LARGE \bf
	Learning to Use Contingency Maneuvers for Robot Navigation in Pedestrian Crowds
}


\author{Patrick Naughton$^{1}$, Ishani Chatterjee$^{2}$, Tushar Kusnur$^{2}$, and Maxim Likhachev$^{2}$% <-this % stops a space
\thanks{$^{1}$Patrick Naughton is with the Electrical and Systems Engineering Department at Washington University in St. Louis, St. Louis, MO, USA. 
        {\tt\small patrickrnaughton@wustl.edu}}%
\thanks{$^{2}$The remaining authors are with the Carnegie Mellon University Robotics Institute, Pittsburgh, PA, USA.
        {\tt\small \{ichatter, tkusnur, mlikhach\}@andrew.cmu.edu}}%
}

\def\tk#1{\textcolor{blue}{Tushar says: #1}}
\begin{document}

\newcolumntype{Z}{S[
	table-format=1.2,% added
	%tight-spacing=true,
	round-mode=places,
	round-precision=2]}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
	When navigating in the presence of humans, mobile robots need robust and efficient ways of dealing with stochastic human behavior. This is often addressed by generating collision-free robot motion based on a model of possible human trajectories. However, such approaches typically assume that the robot's goal is feasible, without considering cases in which human movement may block the robot's path. This can lead to the robot simply stopping, continuously replanning to an infeasible goal, or, in the worst case, colliding with a pedestrian. We address this deficiency by (1) creating a classifier to determine when the robot is likely to fail to reach its goal, and (2) developing a controller to execute a contingency maneuver. This controller safely guides the robot to a different location from which it can replan to its global goal. We evaluate this approach in a simulated scenario where the robot must navigate among human pedestrians and show that it results in fewer collisions than approaches which do not explicitly plan for failures.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
	Dense pedestrian crowds present a challenging navigation environment for a mobile robot. The robot must comply with latent social rules governing its trajectory while simultaneously reaching its goal in a reasonable amount of time. Probabilistic planners, which make navigation plans in the belief space, show potential to make headway in this problem and have demonstrated success dealing with uncertainty \cite{ppcp} like that experienced navigating in a crowd. To form their plans, these planners rely on smaller atomic actions (simple motion primitives or more complex maneuvers) which are strung together to form an overall trajectory. Some of these actions are learned by presenting the robot with its starting pose and a goal pose and running simulations with pedestrians or other agents so that the robot can learn how to interact with the crowd to reach its goal \cite{crowdawarerl}. In general, these controllers guiding each action deal with unexpected situations in which their trajectory becomes infeasible by simply replanning or stopping altogether. Replanning in some cases may take too long to avoid an imminent collision. Additionally, when a waypoint becomes completely unreachable, replanning or stopping may result in a more costly trajectory than simply abandoning it and seeking a safe location from which the planner can generate a new trajectory to the overall goal. We address this issue by developing a \textit{failure controller}---a controller designed to explicitly to navigate in cases where the robot fails to find a plan and no longer has a feasible goal to which it can navigate. The failure controller helps ensure that the robot obeys a safe policy even when it has no waypoint to navigate to. For example, in situations like that in Figure \ref{fig:motivation}, the robot (green circle) attempts to reach a goal (green dot) in the presence of pedestrians (purple circles) and obstacles (black rectangles), but the pedestrians block its path. Staying still (Figure \ref{fig:dnmotivation}) will cause the robot to be a hindrance to the humans and it will likely intrude into their personal space. Moving away to a different area of the scene, as shown in Figure \ref{fig:rlmotivation}, would give the robot a safer place to replan from.
	
	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.49\linewidth}
			\includegraphics[width=\linewidth]{dnmotive}
			\caption{}
			\label{fig:dnmotivation}
		\end{subfigure}
		\begin{subfigure}[t]{0.49\linewidth}
			% \includegraphics{}
			\includegraphics[width=\linewidth]{rlmotive}
			\caption{}
			\label{fig:rlmotivation}
		\end{subfigure}
		\caption{Two situations where a robot (the green circle) is navigating to a goal (the green dot) in the presence of pedestrians (purple circles) and obstacles (black rectangles). As the robot approaches its goal, pedestrians block its planned path. Staying still will result in collisions and intrusions (Fig. \ref{fig:dnmotivation}). Our contingency maneuver safely moves the robot to a new area from which it can replan (Fig. \ref{fig:rlmotivation}).}
		\label{fig:motivation}
	\end{figure}
	
	This work facilitates combining learning with planning for crowd navigation by providing a reinforcement learning based failure controller and a supervised learning based classifier for determining when to use it. Specifically, this paper assumes that an existing planner uses a set of atomic actions to create navigation plans. The set of actions includes some to execute complex behaviors that interact with pedestrians. Success cannot be guaranteed for these controllers because the surrounding humans may interact with the robot or each other in unexpected ways. Note that the failure controller's objective differs from that of a traditional navigation planner in that the failure controller is not given an explicit goal; rather, it attempts to extract the optimal goal and trajectory from its environment.

	The contributions of this work are:
	\begin{enumerate}
		\item A classifier that determines whether or not a robot is likely to reach its goal in a given scene.
		\item A failure controller that safely guides the robot to a position from which it can initiate replanning.
	\end{enumerate}
	The design philosophy taken by this approach separates controllers for atomic actions from the planner. The failure controller and classifier can easily be adapted to work with any planner that uses atomic actions to create a plan.
	
	The remainder of this paper is organized as follows. Section \ref{sec:relatedwork} describes previous work related to the results presented here. Section \ref{sec:problemstatement} outlines our problem statement. Section \ref{sec:approach} describes our approach and Section \ref{sec:experiments} describes the experiments we performed to evaluate our design. Section \ref{sec:results} describes and discusses the results of these experiments and we conclude with Section \ref{sec:conclusion} and discuss future work.
	
	The code of our approach is available here: \texttt{\url{https://github.com/patricknaughton01/LearnController}}.
	

\section{RELATED WORK}\label{sec:relatedwork}
	Several methods use explicit models of human behavior to achieve smooth, predictable robot navigation among pedestrians. Trautman et al. model the interaction between the robot and pedestrians as an extension of an interactive Gaussian process that accommodates multiple goals \cite{caseforcoop}. The social forces model treats humans and the robot in question as masses subject to Newtonian dynamics and applies fictitious forces to them to predict and plan trajectories \cite{sfm}. It recomputes these forces and their effects on robot motion at each time step to determine how the robot should move. These techniques however rely on hand-crafted models of human behavior to achieve their results and handle unexpected or uncooperative human actions by simply replanning using the same model. The social forces model in particular does not demonstrate robust navigation plans and will sometimes exhibit oscillatory behavior in more crowded or narrow areas \cite{sfm}.
	
	Another approach uses inverse reinforcement learning to learn latent, possibly stochastic social rules humans observe when navigating in crowds \cite{socialirl}. This method uses example trajectories recorded from humans or gathered from teleoperated runs. This approach however is extremely unlikely to observe failed trajectories where a human attempts to execute some navigation plan and is forced to completely abort their initial goal. If a human attempts to overtake someone else, for example, they have many contingency options in the case where the other person is either intentionally or unintentionally uncooperative. For example, they could use verbal communication or body language to more explicitly communicate their intentions, options which are not available to many mobile robots. For this reason, inverse reinforcement learning will likely be unable to formulate a useful model for navigation when situations such as these occur.
	
	Reinforcement learning has successfully been applied to the social robot navigation problem using a variety of different models \cite{sociallyawarerl}, \cite{crowdawarerl}. Reinforcement learning is particularly suited to this application as noted in \cite{sociallyawarerl} because it is extremely difficult to specify analytically what the optimal action for a robot to take is, but it is comparatively easy to alert the robot when it performs a socially unacceptable or unsafe action. Previous work has focused on using reinforcement learning to develop policies that generate optimal (in terms of time) paths to a robot's goal in the presence of humans or other autonomous agents. These policies however generally assume the goal is reachable and do not make contingency plans if that assumption turns out to be incorrect. Additionally, the agent is explicitly given a goal to reach by the experimenters; we wish to navigate in the case of failure at which point there is no obvious goal.
	
	The above methods all either deal with failure at execution time by simply replanning or do not consider failure to reach the goal at all. We depart from this paradigm by designing a controller specifically targeted at producing trajectories when the robot's original goal is no longer reachable.
	
\section{PROBLEM STATEMENT}\label{sec:problemstatement}
	We consider a robot that uses a planner to string together different atomic actions to generate a navigation policy to reach some overall goal. Motion primitives are basic actions the robot can take, for example, drive forward one meter. Maneuvers are more complicated actions, for example, barging past a group of people or overtaking someone. These maneuvers can be used by the robot in specific situations to navigate in a scene. We refer to the policy that a robot follows to execute a maneuver as a \textit{controller} and the controller that guides the robot when it initiates a maneuver a \textit{success controller}. This work is concerned with detecting and handling the failure of these controllers. Specifically, we develop a failure controller that corresponds to a given success controller. This failure controller directs the robot if it is determined at execution time that the success controller is unlikely to achieve its goal. 
	
	Additionally, we would like to determine at execution time whether or not the success controller is likely to succeed (i.e. reach its goal). 
	
	Here, we consider one specific controller for barging into a group of people at the end of a corridor. While the experiments and results here only concern this controller, the framework can be extended to include other controllers as well, for example, to overtake or cross in front of pedestrians.
		
\section{APPROACH}\label{sec:approach}
    In this framework, we allow the robot to decide at each time step whether it should begin using the failure controller. Once it switches to the failure controller, it continues navigating using the failure controller until it determines that it can stop (or it exceeds a timeout) and then replans to the overall goal. 
    
    \subsection{State Characterization}
    We characterize the state of the robot using three separate components based on the state used in \cite{crowdawarerl}. The first is the \textit{self state} of the robot, $s^{(s)}=(d_g, s_\text{pref}, \theta, r, v)$ which represents the two-dimensional vector from the robot to its goal, the robot's preferred speed (its maximum speed), the angle of the robot's velocity relative to its heading, the robot's radius, and the two-dimensional velocity vector of the robot with respect to its heading. 
    
    For each pedestrian and obstacle $e_i$ in the scene, we construct an \textit{external state} vector, $s_i^{(e)}=(p_i', v_i', r_i', d_i, r_{\text{sum}_i})$, the relative position and velocity of the other entity, its radius, the center-to-center distance from the robot to this entity, and the sum of the radii of the robot and the entity. For obstacles, we only consider the closest point on the obstacle and assign it a radius of $1\text{e-}7$. We also compute an $s_0^{(e)}$ for the robot itself so that every row of our eventual state matrix has the same number of columns. We define each component the same way, i.e., $p_0'=(0,0)$, $v_0'=v$, etc.
    
    Finally, we construct an occupancy map, $s_i^{(m)}$, around the robot as well as each pedestrian and obstacle point. Each map is a $4\times 4$ grid centered on its respective entity and each grid cell has a side-length of $1$. Each cell stores a boolean variable indicating if it contains an object as well as the average velocity of all objects inside it.
    
    To construct the state of the robot, in a scene with $n$ total entities (the robot, pedestrians, and obstacles) we create an $n\times62$ matrix where each row $i$ takes the form $(s^{(s)}, s_i^{(e)}, s_i^{(m)})$. Note that the size of this input matrix changes depending on the number of pedestrians and obstacles in the scene. It also includes some redundant information, which is used as an attention mechanism to focus the robot's learning \cite{crowdawarerl}.
	
	\subsection{Network Architecture}
	
	This combined state is used as the input to a neural network. We model our architecture off of that presented in \cite{crowdawarerl} in order to handle the variable input sizes. This architecture was shown to produce good results for robotic navigation in the presence of pedestrians in \cite{crowdawarerl}. In addition, we include an LSTM at the end of our network so that the robot can learn relationships between successive states. Figure \ref{fig:model_arch} shows the overall architecture of the model. The output of the network has five nodes which are interpreted as the $x$ and $y$ coordinates of the mean, the $x$ and $y$ standard deviations, and the correlation between $x$ and $y$ of a Gaussian distribution of the next location of the robot conditioned on its current state. At execution time, we then feed this network its current state and attempt to move to the mean of this distribution in the next time step.
	
	\begin{figure}
		\centering
		\includegraphics[height=\linewidth, angle=270]{model_arch.pdf}
		\caption{Model architecture of the neural network used to control the robot. Variable sized inputs are handled by summing a weighted vector across the people and obstacles.}
		\label{fig:model_arch}
	\end{figure}

	We trained this neural network on $1000$ successful trajectories in which the robot can feasibly reach its goal. These trajectories were generated using the RVO2 library \cite{rvo2}\cite{pyrvo2}. We trained using an Adam optimizer with a learning rate of $0.005$ and the negative 2D log-likelihood loss for $500$ epochs. This controller was trained with a very high dropout rate of $0.5$ in each MLP. We do this as a regularization technique and so that at execution time we can sample our network several times to estimate the epistemic uncertainty of our model, which we add to the aleatory uncertainty estimated directly by the network to find the total uncertainty in our prediction \cite{gal2016uncertainty}. Our input and feature MLPs had $80$ output units each, our weight MLP had two hidden layers of $32$ units each and $1$ output unit, our LSTM had $256$ hidden units and the output MLP had two hidden layers with $128$ and $64$ units respectively and an output layer with $5$ units. Each scene contained four pedestrians and two obstacles. Figure \ref{fig:success} shows an example of a trajectory generated by the final controller. 

	\begin{figure}
		\centering
		\includegraphics[width=\linewidth]{success_without_ellipses}
		\caption{Example of a successful trajectory. When executing the barge in success controller, the robot expects the humans in the scene to disperse to let it through.}
		\label{fig:success}
	\end{figure}
	
	We split the approach of the failure controller into two parts. First, we consider just determining whether or not the robot should switch to its failure controller. Given this classifier, we then develop a failure controller that, starting from the state at which the classifier reports we have failed, attempts to learn a policy that guides the robot to a safe location from which to replan.
	
	\subsection{Determining When To Switch}
		In order to rigorously determine whether or not the success controller is likely to succeed (and thus, whether or not we should switch to the failure controller), we train an additional neural network that has the same architecture as the success controller. However, it attempts to learn a target function that predicts the distribution of \textit{previous} locations of the robot given the robot's current state and the fact that the robot is on a trajectory that leads to its goal. We call this model a \textit{reverse predictor}. It is trained on $10,000$ trajectories generated by the success controller in scenes where the success controller can reach its goal. This network however does not receive the robot's full state, rather, it only observes $s^{(s)}$. This was done so that we could train the network by only showing it successful trajectories without the uncertainty in its prediction exploding when it observes unsuccessful ones. We trained this network with the Adam optimizer with a learning rate of $0.005$ and the negative 2D log-likelihood loss. Just as with the success controller, we train with a high dropout likelihood of $0.5$. Our input and feature MLPs had $50$ output units each, our weight MLP had two hidden layers of $32$ and $16$ units respectively and $1$ output unit, our LSTM had $32$ hidden units and the output MLP had two hidden layers with $64$ and $32$ units and an output layer with $5$ units. These $5$ output units represent the same things they did for the success controller, but predict the distribution of the robot's previous locations given its current state rather than next locations.
		
		At execution time, we utilize this network's prediction to perform a statistical $p$-test with some user-specified $\alpha$ value. At each time step we construct the error ellipse given by the Gaussian distribution represented by the reverse predictor that contains $1-\alpha$ of the distribution's probability mass, and check to see if the previous robot's center is inside this ellipse. We perform this same test on the robot's current position given the distribution predicted by the success controller in the previous time step. By default, we assume that the trajectory will be successful and switch to the failure controller only if one of these tests fails. Figure \ref{fig:success_ellipses} shows an example of such error ellipses for three successive positions of the robot.
		
		\begin{figure}
			\centering
			\includegraphics[width=\linewidth]{success_ellipses}
			\caption{Example of the error ellipses used to classify trajectories as successes or failures. The robot is represented by a solid circle in each time step and the error ellipses are dashed. Each shade of green is a different time step and the ellipses associated with a given time step are the same color as the robot in that step. Note that in the first and last states there are no previous or next states so these two positions only need to satisfy one test.}
			\label{fig:success_ellipses}
		\end{figure}
	
		We employ this strategy so that we can train the reverse predictor by showing it only successful trajectories. This is desirable for two reasons. First, there are generally many fewer ways for a robot to successfully reach its goal when interacting with pedestrians than for it to fail. By only training on success trajectories, we do not have to find or generate training data for all of these potential methods of failure. This reduces the potential for biasing the classifier to only identify very specific types of failure. Second, it is much easier to find examples of actual humans successfully barging into a group than examples of them failing to do this. This means that only using successful examples as training data will make this system better able to utilize real world human trajectories in the future.
	
	\subsection{Failure Controller}
		For the failure controller, we need to address a somewhat novel formulation of the navigation problem in that the robot now has no specific goal it is trying to reach. Once the failure controller gets invoked, the robot has determined that it is unlikely to reach its goal. We formulate this as a reinforcement-learning problem $\langle\mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \gamma\rangle$ with an infinite set of states $\mathcal{S}$, a finite set of actions $\mathcal{A}$, state transition matrix $\mathcal{P}(s_t, a_t, s_{t+1})$, reward function $\mathcal{R}(s_t, a_t, s_{t+1})$ and discount factor $\gamma \in (0, 1)$ and model the robot as a Markovian agent \cite{suttonandbarto}. We decided on this approach for a few reasons. First, it is difficult to specify in a closed form what exactly the optimal behavior of the robot is in a given scene. Second, in the case of failure, the robot does not have a goal to plan to. However, we can relatively easily specify whether a given action performed by the robot was good or bad and design a heuristic reward function based on these intuitions. Finally, we performed some preliminary experiments that included having the robot remain still in cases of failure or follow a social forces controller. However, these strategies resulted in worse performance than our reinforcement-learning model: staying still caused the robot to incur many collisions and intrusions, and the social forces model simply drove directly away from all other entities but could not determine when it was safe to stop. For these reasons, we selected a reinforcement-learning model to create our failure controller.
		
		We discretized the action space $\mathcal{A}$ of the robot into $35$ different actions. The robot can move at two different speeds (half speed and full speed) in $16$ evenly spaced directions about its heading. It can also change its heading by $\frac{\pi s_{\text{pref}}}{16}$ in the positive or negative direction. Finally, the robot can choose to remain still.
		
		Our reward function is split into three parts: $\mathcal{R}_{\text{collision}}$, $\mathcal{R}_{\text{movement}}$, and $\mathcal{R}_{\text{smoothness}}$. The overall reward the robot observes is simply the sum of these three components. Equations \ref{eq:rcollision}, \ref{eq:rmovement}, and \ref{eq:rsmoothness} show how these values are calculated. In Equation \ref{eq:rcollision}, $d$ refers to the closest distance between two objects, not their center-to-center distances. Note that each term is computed (where appropriate) with respect to each agent and obstacle in the scene. This means that an action that causes a collision with two agents in a time step receives a collision reward of $-2$, not just $-1$. We apply the $\mathcal{R}_{\text{movement}}$ and $\mathcal{R}_{\text{smoothness}}$ rewards so that, other things equal, the robot will prefer to remain stationary and move in straight lines at consistent velocities that are more predictable for humans in the scene.
		
		\begin{equation}\label{eq:rcollision}
			\mathcal{R}_{\text{collision}} = \begin{cases}
				-1      &   d < 0\\
				-0.25   &   0 \leq d < 0.2\\
				0       &   else
			\end{cases}
		\end{equation}
		
		\begin{equation}\label{eq:rmovement}
			\mathcal{R}_{\text{movement}} = \begin{cases}
			0       &   a_t = 0 \quad\text{(remain still)}\\
			-0.01   &   else
			\end{cases}
		\end{equation}
		
		\begin{equation}\label{eq:rsmoothness}
			\mathcal{R}_{\text{smoothness}} = \begin{cases}
			0       &   a_t = a_{t-1} \vee a_{t-1} = 0\\
			-0.01   &   else
			\end{cases}
		\end{equation}
		
		We then define a $Q_{\pi}(a_t, s_t)$ function that represents the total expected discounted return achieved by executing action $a_t$ from state $s_t$ and thereafter following policy $\pi$. 
		
		\begin{equation}\label{eq:q}
			Q_{\pi} = 
			\sum_{k=0}^{\infty}E[\gamma^k \mathcal{R}(s_{t+k}, \pi(a_{t+k} \mid s_{t+k}), s_{t+k+1})]
		\end{equation}
		
		We represent this function with a deep neural network and use the Deep Q-learning algorithm with experience replay developed in \cite{dqn} to obtain an estimate of the optimal $Q$ function. The state input is the same as that of the success controller and reverse predictor except that $d_x$ and $d_y$ are set to $0$ because at this point we have given up on reaching the goal. We utilize an $\epsilon$-greedy policy with an $\epsilon$ that linearly decays from $1.0$ to $0.1$ over $7,500$ transitions. We set $\gamma=0.9$ and train using the Adam optimizer with learning rate $0.001$. Our input and feature MLPs had $80$ and $120$ output units respectively, our weight MLP had two hidden layers of $64$ and $32$ units respectively and $1$ output unit, our LSTM had $128$ hidden units and the output MLP had two hidden layers with $128$ and $64$ units and an output layer with $35$ units. Each unit of the output is the estimated $Q$ value of the associated action.
		
		To train our agent, we ran $500$ episodes in which the robot begins by executing its success controller. However, the humans in the scene move in such a way to prevent the robot from barging in. Namely, they move into the corridor towards the robot rather than dispersing to allow it through. Once the failure classifier detects failure (or the success controller has executed for $15$ time steps), the robot begins executing an $\epsilon$-greedy policy and stores transitions in its replay buffer. We used a confidence value of $0.99$ to detect failure during training. At each time step, the reward achieved by the robot is computed and its policy is updated. After $15$ time steps, the current episode ends and another one is started. The failure controller moves for $15$ time steps regardless of how long the success controller moved or whether or not a failure was actually detected. 
		
	\subsection{Overall Policy}
		Algorithm \ref{algo:exec} summarizes the overall navigation policy of the robot. $C_s$, $P_r$, and $C_f$ are the success controller, reverse predictor, and failure controllers respectively. $t_s$ is the maximum number of time steps the success controller can run for, and $t_f$ is the same for the failure controller. Note that dropout remains on for both the success controller and reverse predictor at inference time. This is so that we can sample the networks' outputs multiple times to find the variance of their predictions and thereby estimate the network's epistemic uncertainty. This is then added to the data uncertainty which the model directly outputs in order to find the total uncertainty in the robot's predictions \cite{gal2016uncertainty}. $S$ is the number of samples to draw from these networks. $c$ is the confidence value to use when performing the $p$-test to determine whether or not the robot has failed. Here, $G^{(c)}(\mu, \Sigma)$ denotes an error ellipse that contains $c$ of the probability mass of $\mathcal{N}(\mu, \Sigma)$. During execution, we set the $\epsilon$ of the failure controller to $0$ so that it behaves purely greedily. Additionally, after the robot (and humans in the scene) declare their goal velocities, our simulator uses the ORCA algorithm to adjust these velocities to attempt to make them safe (i.e., prevent collisions in the next time step) \cite{orca}. We use this functionality so that humans in the scene respond to the presence of the robot (although not entirely realistically).
		
		\begin{algorithm}
			%\SetAlgoLined
			%\DontPrintSemicolon
			\caption{Detect And Handle Failure}
			\label{algo:exec}
			\begin{algorithmic}
			    \State $C_s$, $P_r$, and $C_f$ are the success controller, reverse predictor, and failure controller.
				\Function{Execute}{$t_s, t_f, S, c$}
					\State Initialize $F\leftarrow false$
					\For{$t=1..t_m$}
						\State Observe $s_t$
						\State Sample $S$ times from $C_s$, compute $\mu_s$ and $\Sigma_s$
						\State $G_s \leftarrow \mathcal{N}(\mu_s, \Sigma_s)$
						\State $G_s^{(c)}\leftarrow c$ error ellipse of $G_s$
						\State Move towards $\mu_s$, observe $s_{t+1}$
						\State Sample $S$ times from $P_r$, compute $\mu_r$ and $\Sigma_r$
						\State $G_r \leftarrow \mathcal{N}(\mu_r, \Sigma_r)$
						\State $G_r^{(c)}\leftarrow c$ error ellipse of $G_r$
						\If{$s_{t+1}\notin G^{(c)}_s \vee s_t \notin G^{(c)}_r$}
							\State $F\leftarrow true$
							\State \textbf{break}
						\EndIf
					\EndFor
					\If{$F$}
						\For{$t=1..t_f$}
							\State Observe $s_t$
							\State Compute $\mu_f$ and $\Sigma_f$ from $C_f(s_t)$
							\State Move towards $\mu_f$
						\EndFor
					\EndIf
				\EndFunction
			\end{algorithmic}
		\end{algorithm}
		
\section{EXPERIMENTS}\label{sec:experiments}
	We began by testing the failure detection system to determine if we could accurately distinguish failure scenarios from successful ones. We ran Algorithm \ref{algo:exec} on $1000$ new success scenarios similar to the ones the success controller learned from (we will refer to these as \textit{success scenes}) with $t_s=15, t_f=0, S=20, c=0.99$, where $t_f=0$ because we are only examining the performance of the failure detection and do not care what the robot does after this point in this experiment. 
	
	We then ran a similar experiment to determine how often the failure detector would correctly identify scenes in which the robot cannot reach its goal. To do this, we utilized the same basic scene setup that was used to train the failure controller which we will refer to as \textit{failure scenes}. We ran Algorithm \ref{algo:exec} on $1000$ such scenes using the same parameters as the corresponding experiment with success scenes.

	To evaluate the effectiveness of our overall framework, we ran two sets of experiments. In one, the failure classifier is completely disabled so that the robot continues to execute its success controller regardless of what the people in the scene do. We set $t_s=15, t_f=0, S=20, c=0.99$. The episode ends after $15$ time steps or after the robot reaches its goal, whichever comes first. We ran this experiment on $1000$ failure scenes.
	
	In the other set, the robot begins by executing its success controller and only switches to the failure controller when the $p$-test performed by the reverse predictor fails. For these episodes, we set $t_s=15, t_f=5, S=20, c=0.99$ so that the failure controller has some time to improve its state after the detection system reports a failure. Note that if the failure detection system does not report a failure after $t_s$ time steps, the episode simply ends. Just as with the previous experiment, we ran this one on $1000$ failure scenes. Figure \ref{fig:experimentscomparison} shows a visual comparison between trajectories generated by the robot in the two experiments.
	
	\begin{figure}
		\centering
		\begin{subfigure}[t]{0.48\linewidth}
			\centering
			\includegraphics[width=\linewidth]{failure_without_failure_controller}
			\caption{}
			\label{fig:failure_without_failure_controller}
		\end{subfigure}\hfill
		\begin{subfigure}[t]{0.48\linewidth}
			\centering
			\includegraphics[width=\linewidth]{failure_with_failure_controller}
			\caption{}
		\end{subfigure}
		\caption{Comparison between trajectories generated by simply executing the success controller (a) versus using the failure controller when a failure is detected (b).}
		\label{fig:experimentscomparison}
	\end{figure}

\section{RESULTS}\label{sec:results}

	We found that the failure detection system reported a spurious failure in $41$ out of the $1000$ runs on a success scene, or $4.1\%$ of the time. In failure scenes, we found that the failure detection system reported a failure in $985$ out of $1000$ runs, or $98.5\%$ of the time. This was a surprisingly positive result because neither the reverse predictor nor the success controller were ever trained on a failure trajectory. The overall test accuracy of the failure detection system was $97.2\%$.
	
	To compare the effectiveness of the two navigation frameworks, we used versions of the metrics presented in \cite{metrics} which were designed to evaluate robot motion in crowds. These metrics are the length of the trajectory, time elapsed, angular distance traveled (computed as total change in direction of motion between time steps), the number of collisions incurred and the number of intrusions incurred. Here, the robot is considered intruding on a human if it comes within $0.2$ units of the human. Similarly to how the reward function is computed, we count collisions (and intrusions) in each time step according to the number of humans or obstacles the robot is colliding with in that step. The time a run takes is the time from the start of the episode to when the robot last changes position. This is why not all success runs take the full $15$ seconds and not all failure runs take $20$ seconds. Table \ref{tab:results} summarizes these results where boldface indicates the better result. 
	
	\sisetup{
		detect-weight=true,
		detect-inline-weight=math,
		output-decimal-marker=\textnormal{.},
	}
	
	\begin{table}
		\centering
		\small
		\caption{Metrics comparing trajectories generated by a robot with no failure controller to a robot using the presented controller. The data here are averages across $1000$ runs for both cases.}\label{tab:results}
		\begin{tabu}{X[m, l] ZZ}
			\toprule
			{}&	{No Failure Controller}   & {Failure Controller (Ours)}  \\\midrule
			Length		  	&	\bftabnum 4.08&		4.88\\
			Angle		    &	\bftabnum 4.59&		6.48\\
			Time  			&	13.37&				\bftabnum 8.03\\
			Collisions  	&	0.73&				\bftabnum 0.15\\
			Intrusions  	&	14.32&				\bftabnum 5.08\\
			\midrule
		\end{tabu}
	\end{table}

	We note that the experiments without the failure controller actually achieved shorter path lengths with smaller overall changes in heading. However, as can be seen in Figure \ref{fig:failure_without_failure_controller}, this is likely because the robot simply continues to butt up against pedestrians without making much progress. Additionally, the values achieved using the failure controller are not much larger, meaning the advantage of the success controller on these metrics is relatively insignificant. The robot also reaches its final position faster when employing the failure controller despite traveling farther. This could potentially allow it to execute its long term navigation plans more quickly because it can begin replanning to its overall goal sooner. More importantly, using the failure controller reduces collisions by over four times and intrusions by over two times. This means that the robot behaves much more safely when it can employ the failure controller presented here rather than relying on just its success controller. 

\section{CONCLUSIONS}\label{sec:conclusion}

	This work augments the integration of learning with planning for long-term autonomous navigation. We presented a failure detection system and failure controller that allow a robot to detect when its current action is likely to fail and provide a way to safely navigate in the absence of a goal. Our failure detection system achieved a very high accuracy in discriminating between success and failure scenarios. The use of the failure controller with the detection system also substantially reduced the number of collisions and intrusions the robot incurred when navigating in failure scenes. These contributions can allow a robot to more safely and efficiently navigate in the presence of humans by giving the robot a contingency plan in cases where it cannot execute its planned controller.
	
	In the future, we would like to extend this work to controllers for other types of situations such as crossing in front of or overtaking a pedestrian. We would also like to collect real pedestrian data from densely crowded areas such as subways, and use these recorded trajectories to learn a success controller (and subsequently a reverse predictor and failure controller). These data would allow the robot to better learn how humans actually move in a given scene compared to using the RVO2-generated trajectories. This would hopefully yield navigation that is safer and more reliable than what is currently achieved. Finally, a natural extension of this work would provide some way for the failure controller to detect, at execution time, when it has reached a suitable state for replanning. Currently, we can only tell after running an experiment by examining when the robot decides to make its final move.


%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}
